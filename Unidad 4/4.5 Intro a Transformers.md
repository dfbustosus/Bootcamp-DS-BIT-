# Transformers

## Que son?

**LLM = Large Language Models (LLMs)** están diseñados para computación paralela y resuelven una amplia gama de tareas sin realizar ajustes detallados

Estos modelos pueden realizar un aprendizaje auto-supervisado sobre grandes cantidades de datos utilizando arquitecturas complejas con miles de millones o billones de parámetros.


## Servicios que incluyen LLMs
- Google Cloud AI
- AWS
- Azure
- OpenAI
- Google Colab
- Github Copilot
- HuggingFace
- Meta
- A lot more!

# O(1) time complexity

O(1) significa orden de 1 y esto representa una complejidad de tiempo constante. Orden de 1 significa orden de 1 operación

Para entender esto tomemos un ejemplo. Si utilizamos la siguiente frase

"A David le gustan las manzanas por la tarde pero no por la mañana"

Esta oración tiene $n=12$ palabras distintas: [A,David, le, gustan, las, manzanas, por, la, tarde, pero, no, mañana].

Cualquier problema de comprensión del lenguaje depende del contexto de una palabra. Esto implica que una palabra normalmente no se puede definir sin un contexto apropiado. ¡¡El significado de una palabra puede cambiar en diferentes contextos incluso si solo hay una definición en el diccionario!!

# Attention layer

Si vemos a detalle la palabra "manzanas" tiene diversas relaciones semanticas 

- Dim 1: Asociación entre manzanas y tarde.
- Dim 2: Asociación entre manzanas y mañana.
- Dim 3: Asociación entre David y las manzanas
- Dim 4: Asociación entre David y la tarde
- Dim 5: Asociación entre David y la mañana
- ...........
- Dim Z

En el caso anterior, verá que las `relaciones` se definen por pares (una palabra a otra). Así es exactamente como funciona la `self-attention` en cualquier modelo transformer como `GPT`.

El problema `O(1)` es solo el siguiente> Realizamos **una** operación por palabra `O(1)` para cada palabra para encontrar la relación con otra palabra en un análisis por pares

Si creamos una notación más formal entonces tenemos>

- `n=` representa la longitud de la secuencia, en este caso 12
- `d=` representa el número de dimensiones expresadas en flotantes. Por ejemplo, si `y` es una palabra, podemos representarla en un vector como este `[0.2333, -0.4300, 0.9566, ..., -0.2455]`. Ahora probablemente te estés preguntando por qué este vector y no otro. Bueno, la respuesta es porque el modelo aprende que la palabra `y` está representada por esto después de analizar muchos puntos de datos de texto.

Cuando hablamos de `O(1)` nos referimos a la `complejidad de la memoria`

Por lo tanto, la complejidad computacional total de una capa de atención viene dada por
$O(n^2 * d)$. Dónde
- $n^2$ es la operación por pares (palabra a palabra) de toda la secuencia n
- $d$ representa el número de dimensiones aprendidas por el modelo, por ejemplo 512

# Recurrent layer

A diferencia de la `Attention Layer`, las capas recurrentes no funcionan de manera similar. Son $O(n)$ y eso se debe a que **cuanto más larga sea la secuencia, más memoria se consumirá**. ¿Por qué? Debido a que no aprenden las dimensiones con relaciones por pares, aprenden en una secuencia, por ejemplo:

- Dim a: David
- Dim b: me gusta y David
- Dim c: manzanas y me gusta y David
- Dim d: en y manzanas y le gusta y David
- .............
- Dim Z

Como puede ver, para cada palabra no solo buscamos otra palabra, sino que buscamos varias otras palabras al mismo tiempo. Por lo tanto, el número de dimensiones para una palabra $d$ se multiplica por las dimensiones de una palabra anterior que aprende $d^2$ dimensiones y la complejidad computacional total será $O(n*d^2)$


# Computational time complexity (Attention Layer)

- La capa de Atención utiliza $O(1)$ complejidad del tiempo de memoria, lo que permite una complejidad de tiempo total de $O(n^2*d)$ para realizar un producto escalar entre cada palabra. Eso implica multiplicar la representación $d$ de cada palabra por otra palabra. **Las capas de atención permiten aprender todas las relaciones en una multiplicación de matrices**

- Las capas recurrentes presentan una complejidad temporal total de $O(n*d^2)$ dada la complejidad individual $O(n)$. luego realizar la misma tarea que la capa de atención requerirá más operaciones.

Podemos crear simulaciones sobre la complejidad de la atención y la complejidad del tiempo recurrente utilizando un enfoque conceptual y probar los resultados con CPU, GPU y TPU.

1. **CPU = Unidad Central de Procesamiento** - Componente principal de la computadora. No es eficiente para cálculos grandes
2. **GPU = Unidad de procesamiento de gráficos**: unidad especializada para renderizado de imágenes 3D y tareas complejas de aprendizaje automático, como multiplicaciones de matrices.
3. **TPU = Unidad de procesamiento tensor**: procesador acelerador de cargas de trabajo de aprendizaje automático creado por Google y optimizado por Tensorflow

# Resultados de simulaciiones computacionales 
1. La complejidad del tiempo computacional de la capa de atención es mucho más rápida que la complejidad del tiempo computacional de la capa recurrente
2. El análisis de palabras uno a uno de la atención es adecuado para detectar dependencias a largo plazo.
3. Las capas de atención permiten la multiplicación de matrices con una gran ventaja de las GPU y TPU

La complejidad $O(1)$ que se deriva de la complejidad $O(N^2 *d)$ es el principio fundamental detrás de cualquier LLM, sin importar cuál sea.

# Producción de tokens en AI (AI revolution)
Hay nueve pasos para producir una salida de LLM 

1. **Tokenización:** convierte una secuencia de entrada en tokens usando un tokenizador. Hablaremos de esto en nuevas conferencias.
2. **Entrada del modelo:** pase la secuencia tokenizada al modelo, por ejemplo, GPT
3. **Modelo:** el modelo procesa la entrada usando diferentes capas desde la capa de entrada a través de múltiples capas de transformer hasta la capa de salida (hablaremos sobre esta arquitectura más adelante, no se preocupe)
4. **Generación de salida:** el modelo produce una cantidad bruta de logits de salida dada la secuencia de entrada
5. **Sampler:** convierte los logits en probabilidades usando diferentes hiperparámetros
6. **Selección del siguiente token (Siguiente TS):** el siguiente token se selecciona en función de las probabilidades del muestreador
7. **Adición del siguiente token:** el siguiente token seleccionado se agrega a la secuencia de entrada y se repite el proceso desde el punto 3 hasta que se alcanza el límite máximo de tokens.
8. **Finalización de la generación de tokens (generación de texto):** la generación de texto finaliza cuando se alcanza el límite máximo de tokens o se identifica un token de fin de secuencia.
9. **Reconstrucción de texto:** Tokenizer convierte la secuencia final de tokens nuevamente en una cadena

Brevemente hay tres pasos principales:
1. Entrada = secuencia de entrada como tokens
2. Entrada de procesamiento del modelo
3. Salida = siguiente token agregado al final de la entrada si se alcanza el máximo de tokens